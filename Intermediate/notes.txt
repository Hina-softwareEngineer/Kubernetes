
To convert strings to secrets:
kubectl get secret hellok8s-secret -o yaml

Interactive Pod for debugging:
kubectl exec -it hellok8s-6d7579848d-kzq57 --  sh
kubectl exec -it hellok8s-6d7579848d-kzq57 --  env | grep MESSAGE


Practical Guide to Kubernetes:

- K3ds to create cluster
- To create Pod, kubectl run db --image mongo (imperative way) [Not preferred]
- POD
```
spec:
  containers:
  - name: db
    image: mongo:3.3
    command: ["mongod"]
    args: ["--rest", "--httpinterface"]
```

kubectl get pods -o wide
kubectl describe pod pod-name or kubectl describe -f db.yml



- Logs of container in pods:
kubectl logs pod-name


When we send the instruction to delete a Pod, Kubernetes tries to terminate it gracefully.

The first thing it does is to send the TERM (terminate) signal to all the main processes inside the containers that form the Pod.

From there on, Kubernetes gives each container a period of thirty seconds so that the processes in those containers can shut down properly.

Once the grace period expires, the KILL signal is sent to terminate all the main processes forcefully and, with them, all the containers. The default grace period can be changed through the gracePeriodSeconds value in the YAML definition or --grace-period argument of the kubectl delete command.

Filter the yml file:
kubectl get -f go-demo-2.yml \
    -o jsonpath="{.spec.containers[*].name}"

Let’s go back to our original multi-container Pod that defined the api and db containers. That was a terrible design choice because it tightly coupled one with the other. As a result, when we explore how to scale Pods (not containers), both would need to match. If, for example, we scale the Pod to three, we’d have three APIs and three DBs. Instead, we should have defined two Pods, one for each container (db and api). That would give us enough flexibility to treat each independently from the other.




There are scenarios when having multiple containers in a Pod is a good idea. However, they are very specific and, in most cases, are based on one container that acts as the main service and the rest that serve as side-cars.

Multi-container Pods are frequently used for:

Continuous integration (CI)
Continuous Delivery (CD)
Continuous Deployment processes (CDP)

- To monitor health, we can exploit Kubernetes liveness and readiness probes.

Liveness probe
livenessProbe can be used to confirm whether a container should be running. If the probe fails, Kubernetes will terminate the container and apply a restart policy which defaults to Always.

Readiness probe
We’ll leave readinessProbe for later since it is directly tied to Services.

Instead, we’ll explore livenessProbe. Both are defined in the same way so the experience with one of them can be easily applied to the other.

```
 livenessProbe:
      httpGet:
        path: /this/path/does/not/exist
        port: 8080
      initialDelaySeconds: 5
      timeoutSeconds: 2 # Defaults to 1
      periodSeconds: 5 # Defaults to 10
      failureThreshold: 1 # Defaults to 3
```

We declare that the first execution of the probe should be delayed by five seconds (initialDelaySeconds). The requests should timeout after two seconds (timeoutSeconds). We also declare that the process should be repeated every five seconds (periodSeconds), and (failureThreshold) define how many attempts it must try before giving up.
On failure, it restarts the container.


REPLICASETS:
ReplicaSet’s primary function is to ensure that the specified number of replicas of service are (almost) always running.

kubectl get rs
kubectl delete -f go-demo-2.yml --cascade=orphan

kubectl get pods --show-labels
Removing Label from pod: kubectl label $POD_NAME service-
Add Label to POD: kubectl label $POD_NAME service=go-demo-2

SERVICES:
```
kubectl expose rs go-demo-2 \
    --name=go-demo-2-svc \
    --target-port=28017 \
    --type=NodePort
```
NodePort: the target port will be exposed on every node of the cluster to the outside world and be routed to one of the Pods controlled by the ReplicaSet.

ClusterIP (the default type) exposes the port only inside the cluster. Such a port would not be accessible from anywhere outside. ClusterIP is useful when we want to enable communication between Pods and still prevent any external access.

The LoadBalancer type is only useful when combined with the cloud provider’s load balancer.

Next is the NodePort type which exposes ports to all the nodes. Since NodePort automatically creates ClusterIP type as well, all the Pods in the cluster can access the TargetPort. The Port is set to 28017. That is the port that the Pods can use to access the Service. NodePort is generated automatically since we do not set it explicitly. It is the port that we can use to access the Service and, therefore, the Pods from outside the cluster. In most cases, it should be randomly generated. That way, we avoid any clashes.

Port exposes the Kubernetes service on the specified port within the cluster. Other pods within the cluster can communicate with this server on the specified port.
TargetPort is the port on which the service will send requests to, that your pod will be listening on. Your application in the container will need to be listening on this port also.
NodePort exposes a service externally to the cluster by means of the target nodes IP address and the NodePort. NodePort is the default setting if the port field is not specified.





